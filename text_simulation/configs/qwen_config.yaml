# Qwen2.5-7B-Instruct Configuration for Digital Twin Simulation
# Qwen is a standard autoregressive language model that runs locally (not via API)

# Model configuration
model_name: "Qwen/Qwen2.5-7B-Instruct"
device: "cuda"  # Use "cpu" if no GPU available (will be very slow)

# Generation parameters (Qwen-specific)
temperature: 0.2  # Lower temperature for more deterministic responses (good for survey simulation)
max_new_tokens: 512  # Maximum tokens to generate
top_p: 0.95  # Nucleus sampling
top_k: null  # Top-k sampling (null to disable)

# Context management
max_context_length: 32768  # Qwen2.5 has 32K context limit (input + output)

# Processing options
max_retries: 10
num_workers: 1  # Keep at 1 for local model (GPU memory limited)
force_regenerate: false
max_personas: -1  # Set to -1 or null for all personas, or specify a number to limit

# Input/Output directories
input_folder_dir: "text_simulation_input"
output_folder_dir: "text_simulation_output_qwen"  # Separate output dir for Qwen

# System instruction for behavioral predictions
system_instruction: |
  You are an AI assistant that predicts human behavior based on demographic characteristics and psychographic traits. 
  Given demographic information and psychographic measures (Need for Cognition, Spendthrift/Tightwad, Maximization, Minimalism), predict how a person would respond to behavioral questions. 
  Respond with only a single number corresponding to the option they would choose.

# Qwen-specific notes:
# - Requires transformers and torch (standard versions)
# - Needs at least 14GB GPU memory (7B model)
# - Context length is 32K tokens (much larger than Dream/LLaDA)
# - Processing is slower than API models but runs locally (no API costs)
# - Uses standard autoregressive generation (not diffusion-based)

